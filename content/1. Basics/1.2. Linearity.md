---
title: 1.2. Linearity
draft: false
date: 2023-12-25
---

The concept of linearity is quite a general concept. However, I will keep it limited to linear transformation in the context of functions and matrix multiplications.

## Linearity in Functions
----
Consider a function $f$. For linearity, two different properties need to hold. 

- Additivity: $f(x+y)=f(x)+f(y)$
- Homogeneity of degree-1 (Scaling): $f(ax) = af(x)$

Now these two properties can be combined into a single expression that holds if a function is linear:

$$
f(\alpha x+ \beta y) = \alpha f(x) + \beta f(y)
$$

## Linearity in terms of Matrix Multiplication
-----
The map $\mathbf{x} \rightarrow f(\mathbf{x})$ is linear if for any $\mathbf{x}, \mathbf{y} \in \mathbb{C}^{n}$ and any $\alpha \in \mathbb{C}$,


$$
\boxed{\begin{aligned} \\
\quad \mathbf{A}(\mathbf{x}+\mathbf{y}) & =\mathbf{A}  \mathbf{x}+\mathbf{A} \mathbf{y} \quad \\
\mathbf{A}(\alpha \mathbf{x}) & =\alpha \mathbf{A} \mathbf{x}
\\
\\
\end{aligned}}
$$

What does it mean intuitively or visually?  It is hard to describe its general form. But some properties that need to be preserved in geometric sense:

- After the transformation, the origin of the vector can not be shifted 
- Parallel lines stays parallel after transformation. 

I will provide two counter-examples where <b>linearity is not preserved. </b>

The following is an example of an affine transformations.
$$
f(\mathbf{x})= \mathbf{A}\mathbf{x}+ \mathbf{b}

$$
These kinds of transformations perform an additional "shifting" to the vectors in addition to a linear transformation.  Here, the shifting is done by the vector $\mathbf{b}$ and essentially shifts the origin which violates the assumptions of linearity.

The following is another visualization of how a non-linear transformation may look like.

![[Pasted image 20231227065924.png]]
## Understanding the  Linearity in Matrix Multiplication
----
Let's define the matrix transformation $\mathbf{A}$ as some functional transformation to the elementary basis vectors of the vector space $\mathbb{R}^n$.

$$

\mathbf{A}= \left[ f(\mathbf{e_1})~|~f(\mathbf{e_2})|~\cdots ~ | ~ f(\mathbf{e_n})\right]

$$

What happens when we multiply the matrix $\mathbf{A}$ with a specific basis vector? 

$$
\begin{align*}
\mathbf{A} \mathbf{e_i} 
& =\left[ f(\mathbf{e_1})~|~f(\mathbf{e_2})|~\cdots ~ | ~ f(\mathbf{e_n})\right]~ \mathbf{e_i} \\
& =f(\mathbf{e_i}) 
\end{align*}
$$

What happens when we do this for  a general vector $\mathbf{x}$, 

$$
\begin{align*}
f(\mathbf{x}) &=f \left(\begin{bmatrix} x_1 \\x_2 \\ \vdots \\ x_n\end{bmatrix}\right) \\
& = f(x_1 \mathbf{e_1}+ x_2\mathbf{e_2}+ \ldots+x_n\mathbf{e_n} )
\end{align*}
$$
Now assuming the linearity holds we can we write the following

$$
\begin{align*}
f(\mathbf{x}) 
& = f(x_1 \mathbf{e_1}+ x_2\mathbf{e_2}+ \ldots+x_n\mathbf{e_n} ) \\
& = x_1 f(\mathbf{e_1})+ x_2 f(\mathbf{e_2})+ \ldots+ x_n f(\mathbf{e_n}) \\
& = \left[ f(\mathbf{e_1})~|~f(\mathbf{e_2})|~\cdots ~ | ~ f(\mathbf{e_n})\right] \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \\
& = \left[ f(\mathbf{e_1})~|~f(\mathbf{e_2})|~\cdots ~ | ~ f(\mathbf{e_n})\right]~\mathbf{x} \\
& = \mathbf{A} \mathbf{x}
\end{align*}
$$



