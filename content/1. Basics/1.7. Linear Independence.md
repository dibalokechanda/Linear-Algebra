
## Definition 

A set of vectors $S=\{ \mathbf{v_{1}}, \mathbf{v_{2}},\cdots, \mathbf{v_{n}}\}$  is linearly independent if the following is true:

$$
\alpha_{1}\mathbf{v_{1}}+\alpha_{2}\mathbf{v_{2}}+\cdots+\alpha_{n}\mathbf{v_{n}=0 }~\text{iff}~ \alpha_{1}, \alpha_{2}, \cdots,\alpha_{n}=0
$$

To interpret this we just need to think about a counter example. Consider a set of vectors $\{\mathbf{v_{i}}\}$ and another set of vectors $\{\mathbf{v_{j}}\}$ from $S$. Now the linear combination of the first set of vectors $\{ \alpha_{i} \mathbf{v_{i}} \}$ is in direction opposition of the linear combination of vectors $\{ \alpha_{j} \mathbf{v_{j}}\}$ which results in $\mathbf{0}$. That would mean there is a linear dependence. In what scenario there wouldn't be a linear dependence? 
If no vector (linear of combination of vectors) from $S$ can be expressed as a linear combination of any other vector/vectors.

Another definition which relates the linear independence to the concept of [[1.6. Span|Span]] is mentioned below. For any vector $\mathbf{v}_{i}\in S$ the following needs to hold true: 

$$
\mathbf{v}_i \notin \operatorname{Span}\left(S \backslash\left\{\mathbf{v}_i\right\}\right)
$$

## References

1.  Bernstein, Matthew.Â _Span and Linear Independence_. 11 June 2022, mbernste.github.io/posts/linear_independence/.








