---
title: 1.1. Matix Vector Multiplication
draft: false
date: 2023-12-23
---

Consider a matrix $\mathbf{A} \in \mathbb{C}^{m\times n}$ and a vector $\mathbf{x} \in \mathbb{C}^{n}$. 

 $$
\mathbf{A}=\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
\mathbf{a_1} & \mathbf{a_2} & \ldots & \mathbf{a_n} \\
\mid & \mid & & \mid
\end{array}\right]
$$
  
where, $\mathbf{a_1},\mathbf{a_2},\ldots,\mathbf{a_n}$ are columns of matrix $\mathbf{A}$. 

$$

\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$


There are several ways we can interpret the Matrix vector multiplication. 


## Linear Combination of Columns of Matrix 
----
A way to express it is scaling the columns of $\mathbf{A}$ with the elements of vector $\mathbf{x}$.



$$
\begin{align*}
\mathbf{A}\mathbf{x} & = \left[\begin{array}{cccc}
\mid & \mid & & \mid \\
\mathbf{a_1} & \mathbf{a_2} & \ldots & \mathbf{a_n} \\
\mid & \mid & & \mid
\end{array}\right] \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \\ \\&= x_1 \mathbf{a_1}+x_2\mathbf{a_2}+ \ldots +x_n\mathbf{a_n}
\end{align*}
$$


This can be written in a much more compact form as follows:

$$
\mathbf{A}\mathbf{x}= x_1 \mathbf{a_1}+x_2\mathbf{a_2}+ \ldots +x_n\mathbf{a_n}= \sum_{i=1}^{n}x_{1} \mathbf{a_1}
$$

This is nothing but a linear combination of the columns of matrix $\mathbf{A}$. 


## Functional Transformation Perspective
----
We can interpret the matrix-vector multiplication through the lens of functions.

Consider a function $f:\mathbb{R}^{n} \rightarrow \mathbb{R}^m$ and the matrix-vector multiplication can be thought of as the following transformation:

$$
f(\mathbf{x})=\mathbf{A} \mathbf{x}
$$

To put it into words, we are plugging in a vector $\mathbf{x} \in \mathbb{R}^{n}$ which results in applying a transformation defined by a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$. The result of the transformation is another vector in $\mathbb{R}^{m}$.  This can be further interpreted as a mapping between two vector spaces; from $\mathbb{R}^{n}$ to  $\mathbb{R}^{m}$.

![[Pasted image 20231231073628.png]]

Note that this holds true for $f:\mathbb{C}^{n} \rightarrow \mathbb{C}^m$ also. 

Most of the time we do not separate out the input space and the output space of the function but superimpose them in a single plot. This allows us to visualize the transformation in a single plot. 

Now what kind of transformation would it be depends on the nature of the 
matrix. Visualization and intuition behind these transformations are explained in detail in [[1.3. Matrix Multiplication as Transformation|Matrix Multiplication as Transformation]].

Another important thing to mention here, these transformations require one specific property which is the function needs to be linear. In other words, these transformations need to hold the [[1.2. Linearity|Linearity]] property.

## Establishing the relation with elementary basis vectors 
---
Rather than thinking about the matrix $\mathbf{A}$ transforming the vector $\mathbf{x}$, we can think about it from another perspective. To do this we need to decompose the vector $\mathbf{x}$ into elementary basis vectors.

$$
\mathbf{x} = x_1 \mathbf{e_1}+x_2 \mathbf{e_2}+ \cdots+ x_n \mathbf{e_n}
$$

where, $\mathbf{e_1},\mathbf{e_2},\cdots, \mathbf{e_n}$ are the elementary basis vectors.

For better understanding let's write out the expression more explicitly,

$$
\mathbf{x} = x_1 \begin{bmatrix} 1 \\0\\ 0 \\ \vdots \\ 0\end{bmatrix}+ x_2 \begin{bmatrix} 0 \\1\\ 0 \\ \vdots \\ 0\end{bmatrix}+ x_3 \begin{bmatrix} 0 \\0\\ 1 \\ \vdots \\ 0\end{bmatrix}+ \ldots+  x_n \begin{bmatrix} 0 \\0\\ 0 \\ \vdots \\ 1\end{bmatrix}
$$
Visualize this as decomposing the vector $\mathbf{x}$ into it's elementary basis vectors.  Each elementary basis vector is getting scaled by $x_1$, $x_2, \ldots, x_n$ and summed up to form the vector $\mathbf{x}$.  

Now, when we multiply the matrix with the vector $\mathbf{x}$, what actually gets transformed is the "elementary basis vectors".  Let's see it mathematically 

$$
\begin{align*}
\mathbf{A} \mathbf{x} &=\mathbf{A}(x_1 \mathbf{e_1}+x_2 \mathbf{e_2}+ \cdots+ x_n \mathbf{e_n}) \\
&= x_1 \mathbf{A} \mathbf{e_1}+ x_2 \mathbf{A} \mathbf{e_2}+\ldots +x_n \mathbf{A} \mathbf{e_n}
\end{align*}
$$
Note from going to the first line to the second line requires a strong assumption which is [[1.2. Linearity|Linearity]]. 

Think of  $\mathbf{A} \mathbf{e_i}$ as just applying the transformation defined by matrix $\mathbf{A}$ to the elementary basis vector $\mathbf{e_i}$. But notice what happens when we actually go through the computation:

$$
\mathbf{A} \mathbf{e_1}= \left[\begin{array}{cccc}
\mid & \mid & & \mid \\
\mathbf{a_1} & \mathbf{a_2} & \ldots & \mathbf{a_n} \\
\mid & \mid & & \mid
\end{array}\right]\begin{bmatrix} 1 \\0\\ 0 \\ \vdots \\ 0\end{bmatrix} = \mathbf{a_1}
$$
$$
\mathbf{A} \mathbf{e_2}= \left[\begin{array}{cccc}
\mid & \mid & & \mid \\
\mathbf{a_1} & \mathbf{a_2} & \ldots & \mathbf{a_n} \\
\mid & \mid & & \mid
\end{array}\right]\begin{bmatrix} 0 \\1\\ 0 \\ \vdots \\ 0\end{bmatrix} = \mathbf{a_2}
$$
$$
\vdots
$$
$$
\mathbf{A} \mathbf{e_n}= \left[\begin{array}{cccc}
\mid & \mid & & \mid \\
\mathbf{a_1} & \mathbf{a_2} & \ldots & \mathbf{a_n} \\
\mid & \mid & & \mid
\end{array}\right]\begin{bmatrix} 0 \\0\\ 0 \\ \vdots \\ 1\end{bmatrix} = \mathbf{a_n}
$$

We are just pulling out the corresponding column (index of $\mathbf{e_i}$) of the matrix $\mathbf{A}$. And, we get back the first perspective of the matrix-vector multiplication i.e. linear combination of columns of a matrix.

> [!important] How to Interpret a Matrix
> The columns of matrix $\mathbf{A}$  specify where elementary basis vectors will end up after transformation.
>
>$$
>
> \mathbf{e_i} \rightarrow \mathbf{a_i}
> $$
> Here, we specifying $\mathbf{e_i}$ will be transformed to $\mathbf{a_i}$

## Inner Product Perspective 
----
For this perspective we need to express the matrix $\mathbf{A}$ as follows:


$$
\mathbf{A}=\left[\begin{array}{ccc}
- & \mathbf{a_1}^{\prime^\top} & - \\

- & \mathbf{a_2}^{\prime^\top} & - \\

& \vdots & \\

-& \mathbf{a_n}^{\prime^\top} & -
\end{array}\right]
$$

where, $\mathbf{a^\prime_1},\mathbf{a^\prime_2},\cdots,\mathbf{a^\prime_n}$ are the rows of matrix $\bf{A}$.

Considering this, the matrix-vector multiplication can be expressed as the following:
$$
\mathbf{A}\mathbf{x} = \left[ \begin{array}{c} \mathbf{a^{\prime^\top}_1}\mathbf{x }  \\\  \mathbf{a^{\prime^\top}_2}\mathbf{x }\ \\ \vdots \\ \mathbf{a^{\prime^\top}_n}\mathbf{x}\end{array} \right]
$$
**Side Note:**
Just as a caution, you have to be careful how you write-out the matrix $\mathbf{A}$. If it is written in the following form,
$$
\mathbf{A}=\left[\begin{array}{ccc}
- & \mathbf{a_1}^{\prime} & - \\

- & \mathbf{a_2}^{\prime} & - \\

& \vdots & \\

-& \mathbf{a_n}^{\prime} & -
\end{array}\right]
$$

In matrix vector multiplication we need to write the inner product explicitly as follows:

$$
\mathbf{A}\mathbf{x} = \left[ \begin{array}{c} \langle\mathbf{a^{\prime}_1},\mathbf{x } \rangle \\\  \langle\mathbf{a^{\prime}_2},\mathbf{x }\rangle\ \\ \vdots \\ \langle\mathbf{a^{\prime}_n},\mathbf{x}\rangle\end{array} \right]
$$


## References 
----
 1. Gundersen, Gregory. _Matrices as Functions, Matrices as Data_. 28 Aug. 2022, gregorygundersen.com/blog/2022/08/28/matrices-as-functions-and-data/.
 2. Bernstein, Matthew. _Matrix-vector multiplication_. 19 Dec. 2020, mbernste.github.io/posts/matrix_vector_mult.